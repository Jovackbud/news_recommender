{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce48621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T03:25:42.503096Z",
     "iopub.status.busy": "2025-05-31T03:25:42.502823Z",
     "iopub.status.idle": "2025-05-31T03:25:45.163086Z",
     "shell.execute_reply": "2025-05-31T03:25:45.162215Z"
    },
    "id": "6rWKZp3pZcbL",
    "outputId": "7c396a4b-db87-4380-a100-27eb235a98cd",
    "papermill": {
     "duration": 2.665543,
     "end_time": "2025-05-31T03:25:45.164189",
     "exception": false,
     "start_time": "2025-05-31T03:25:42.498646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths defined:\n",
      "MINDsmall dev: /kaggle/input/mind-small/dataset/MINDsmall_dev\n",
      "MINDsmall train: /kaggle/input/mind-small/dataset/MINDsmall_train\n",
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle # For saving objects\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths to the dataset\n",
    "MINDSMALL_DEV_PATH = '/kaggle/input/mind-small/dataset/MINDsmall_dev'\n",
    "MINDSMALL_TRAIN_PATH = '/kaggle/input/mind-small/dataset/MINDsmall_train'\n",
    "\n",
    "# Check if paths exist (optional, but good practice)\n",
    "if not os.path.exists(MINDSMALL_DEV_PATH):\n",
    "    print(f\"Development path not found: {MINDSMALL_DEV_PATH}\")\n",
    "if not os.path.exists(MINDSMALL_TRAIN_PATH):\n",
    "    print(f\"Training path not found: {MINDSMALL_TRAIN_PATH}\")\n",
    "\n",
    "print(\"Paths defined:\")\n",
    "print(f\"MINDsmall dev: {MINDSMALL_DEV_PATH}\")\n",
    "print(f\"MINDsmall train: {MINDSMALL_TRAIN_PATH}\")\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2058ddfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T03:25:45.170389Z",
     "iopub.status.busy": "2025-05-31T03:25:45.170101Z",
     "iopub.status.idle": "2025-05-31T03:25:49.024397Z",
     "shell.execute_reply": "2025-05-31T03:25:49.023517Z"
    },
    "id": "ajnV0QlEAyOy",
    "outputId": "f8864252-6ddd-467c-f0f1-8f723c9665d9",
    "papermill": {
     "duration": 3.858214,
     "end_time": "2025-05-31T03:25:49.025315",
     "exception": false,
     "start_time": "2025-05-31T03:25:45.167101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train news data loaded successfully from /kaggle/input/mind-small/dataset/MINDsmall_train/news.tsv\n",
      "Train News Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51282 entries, 0 to 51281\n",
      "Columns: 8 entries, NewsID to AbstractEntities\n",
      "dtypes: object(8)\n",
      "memory usage: 61.3 MB\n",
      "\n",
      "Train News Data Head (first 3 rows):\n",
      "   NewsID   Category      SubCategory  \\\n",
      "0  N55528  lifestyle  lifestyleroyals   \n",
      "1  N19639     health       weightloss   \n",
      "2  N61837       news        newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1                      50 Worst Habits For Belly Fat   \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  These seemingly harmless habits are holding yo...   \n",
      "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "2                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Train behaviors data loaded successfully from /kaggle/input/mind-small/dataset/MINDsmall_train/behaviors.tsv.\n",
      "Train Behaviors Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156965 entries, 0 to 156964\n",
      "Columns: 5 entries, ImpressionID to Impressions\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 121.5 MB\n",
      "\n",
      "Train Behaviors Data Head (first 3 rows):\n",
      "   ImpressionID  UserID                   Time  \\\n",
      "0             1  U13740  11/11/2019 9:05:58 AM   \n",
      "1             2  U91836  11/12/2019 6:11:30 PM   \n",
      "2             3  U73700  11/14/2019 7:01:48 AM   \n",
      "\n",
      "                                             History  \\\n",
      "0  N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n",
      "1  N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
      "2  N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
      "\n",
      "                                         Impressions  \n",
      "0                                  N55689-1 N35729-0  \n",
      "1  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  \n",
      "2  N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...  \n"
     ]
    }
   ],
   "source": [
    "# These paths should be defined if Cell 1 ran successfully.\n",
    "# MINDSMALL_TRAIN_PATH = '/content/drive/MyDrive/news_recomender/MINDsmall_train' # Defined in Cell 1\n",
    "# MINDSMALL_DEV_PATH = '/content/drive/MyDrive/news_recomender/MINDsmall_dev'     # Defined in Cell 1\n",
    "\n",
    "# Define column names for news.tsv\n",
    "news_cols = ['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities']\n",
    "\n",
    "# Load train news.tsv\n",
    "train_news_file = os.path.join(MINDSMALL_TRAIN_PATH, 'news.tsv')\n",
    "try:\n",
    "    train_news_df = pd.read_csv(\n",
    "        train_news_file,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=news_cols\n",
    "    )\n",
    "    print(f\"Train news data loaded successfully from {train_news_file}\")\n",
    "    print(\"Train News Data Info:\")\n",
    "    train_news_df.info(verbose=False, memory_usage='deep') # verbose=False for brevity\n",
    "    print(\"\\nTrain News Data Head (first 3 rows):\")\n",
    "    print(train_news_df.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {train_news_file}. Please ensure MINDSMALL_TRAIN_PATH is correct and the file exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {train_news_file}: {e}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Define column names for behaviors.tsv\n",
    "behavior_cols = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    "\n",
    "# Load train behaviors.tsv\n",
    "train_behaviors_file = os.path.join(MINDSMALL_TRAIN_PATH, 'behaviors.tsv')\n",
    "try:\n",
    "    train_behaviors_df = pd.read_csv(\n",
    "        train_behaviors_file,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=behavior_cols\n",
    "    )\n",
    "    print(f\"\\nTrain behaviors data loaded successfully from {train_behaviors_file}.\")\n",
    "    print(\"Train Behaviors Data Info:\")\n",
    "    train_behaviors_df.info(verbose=False, memory_usage='deep') # verbose=False for brevity\n",
    "    print(\"\\nTrain Behaviors Data Head (first 3 rows):\")\n",
    "    print(train_behaviors_df.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {train_behaviors_file}. Please ensure MINDSMALL_TRAIN_PATH is correct and the file exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {train_behaviors_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12d46ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T03:25:49.031358Z",
     "iopub.status.busy": "2025-05-31T03:25:49.031157Z",
     "iopub.status.idle": "2025-05-31T03:25:50.797403Z",
     "shell.execute_reply": "2025-05-31T03:25:50.796504Z"
    },
    "id": "YZqr8GkLanEF",
    "outputId": "a5e265f2-f111-4179-def5-bbb711330d19",
    "papermill": {
     "duration": 1.770335,
     "end_time": "2025-05-31T03:25:50.798450",
     "exception": false,
     "start_time": "2025-05-31T03:25:49.028115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev news data loaded successfully from /kaggle/input/mind-small/dataset/MINDsmall_dev/news.tsv\n",
      "Dev News Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42416 entries, 0 to 42415\n",
      "Columns: 8 entries, NewsID to AbstractEntities\n",
      "dtypes: object(8)\n",
      "memory usage: 50.2 MB\n",
      "\n",
      "Dev News Data Head (first 3 rows):\n",
      "   NewsID   Category      SubCategory  \\\n",
      "0  N55528  lifestyle  lifestyleroyals   \n",
      "1  N18955     health          medical   \n",
      "2  N61837       news        newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1  Dispose of unwanted prescription drugs during ...   \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1                                                NaN   \n",
      "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "2                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1                                                 []  \n",
      "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Dev behaviors data loaded successfully from /kaggle/input/mind-small/dataset/MINDsmall_dev/behaviors.tsv.\n",
      "Dev Behaviors Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73152 entries, 0 to 73151\n",
      "Columns: 5 entries, ImpressionID to Impressions\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 56.6 MB\n",
      "\n",
      "Dev Behaviors Data Head (first 3 rows):\n",
      "   ImpressionID  UserID                    Time  \\\n",
      "0             1  U80234  11/15/2019 12:37:50 PM   \n",
      "1             2  U60458   11/15/2019 7:11:50 AM   \n",
      "2             3  U44190   11/15/2019 9:55:12 AM   \n",
      "\n",
      "                                             History  \\\n",
      "0  N55189 N46039 N51741 N53234 N11276 N264 N40716...   \n",
      "1  N58715 N32109 N51180 N33438 N54827 N28488 N611...   \n",
      "2  N56253 N1150 N55189 N16233 N61704 N51706 N5303...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N28682-0 N48740-0 N31958-1 N34130-0 N6916-0 N5...  \n",
      "1  N20036-0 N23513-1 N32536-0 N46976-0 N35216-0 N...  \n",
      "2  N36779-0 N62365-0 N58098-0 N5472-0 N13408-0 N5...  \n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Development (Validation) Data\n",
    "\n",
    "# Ensure pandas and os are imported. If this cell is run in a fresh session without running prior cells:\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# Paths MINDSMALL_DEV_PATH should be defined from Cell 1.\n",
    "# If not defined because Cell 1 wasn't run in this session, you would need to define it:\n",
    "# MINDSMALL_DEV_PATH = '/content/drive/MyDrive/news_recomender/MINDsmall_dev'\n",
    "\n",
    "# Column names (same as training data)\n",
    "news_cols = ['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities']\n",
    "behavior_cols = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    "\n",
    "# Load dev news.tsv\n",
    "dev_news_file = os.path.join(MINDSMALL_DEV_PATH, 'news.tsv')\n",
    "try:\n",
    "    dev_news_df = pd.read_csv(\n",
    "        dev_news_file,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=news_cols\n",
    "    )\n",
    "    print(f\"Dev news data loaded successfully from {dev_news_file}\")\n",
    "    print(\"Dev News Data Info:\")\n",
    "    dev_news_df.info(verbose=False, memory_usage='deep')\n",
    "    print(\"\\nDev News Data Head (first 3 rows):\")\n",
    "    print(dev_news_df.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {dev_news_file}. Please ensure MINDSMALL_DEV_PATH is correct and the file exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {dev_news_file}: {e}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Load dev behaviors.tsv\n",
    "dev_behaviors_file = os.path.join(MINDSMALL_DEV_PATH, 'behaviors.tsv')\n",
    "try:\n",
    "    dev_behaviors_df = pd.read_csv(\n",
    "        dev_behaviors_file,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=behavior_cols\n",
    "    )\n",
    "    print(f\"\\nDev behaviors data loaded successfully from {dev_behaviors_file}.\")\n",
    "    print(\"Dev Behaviors Data Info:\")\n",
    "    dev_behaviors_df.info(verbose=False, memory_usage='deep')\n",
    "    print(\"\\nDev Behaviors Data Head (first 3 rows):\")\n",
    "    print(dev_behaviors_df.head(3))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {dev_behaviors_file}. Please ensure MINDSMALL_DEV_PATH is correct and the file exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {dev_behaviors_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94961166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T03:25:50.805374Z",
     "iopub.status.busy": "2025-05-31T03:25:50.804808Z",
     "iopub.status.idle": "2025-05-31T03:25:51.034733Z",
     "shell.execute_reply": "2025-05-31T03:25:51.033987Z"
    },
    "id": "VfZilhoTaqz4",
    "outputId": "50394c1e-782c-4c4f-a7f9-3b42070e9e7d",
    "papermill": {
     "duration": 0.234335,
     "end_time": "2025-05-31T03:25:51.035789",
     "exception": false,
     "start_time": "2025-05-31T03:25:50.801454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train_news_df...\n",
      "Train News Data After Preprocessing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51282 entries, 0 to 51281\n",
      "Columns: 9 entries, NewsID to CombinedText\n",
      "dtypes: object(9)\n",
      "memory usage: 77.7 MB\n",
      "\n",
      "Train News Data Head (first 3 rows with CombinedText):\n",
      "   NewsID   Category      SubCategory  \\\n",
      "0  N55528  lifestyle  lifestyleroyals   \n",
      "1  N19639     health       weightloss   \n",
      "2  N61837       news        newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1                      50 Worst Habits For Belly Fat   \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  These seemingly harmless habits are holding yo...   \n",
      "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                        CombinedText  \n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...  \n",
      "1  50 Worst Habits For Belly Fat These seemingly ...  \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Preprocessing dev_news_df...\n",
      "Dev News Data After Preprocessing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42416 entries, 0 to 42415\n",
      "Columns: 9 entries, NewsID to CombinedText\n",
      "dtypes: object(9)\n",
      "memory usage: 63.4 MB\n",
      "\n",
      "Dev News Data Head (first 3 rows with CombinedText):\n",
      "   NewsID   Category      SubCategory  \\\n",
      "0  N55528  lifestyle  lifestyleroyals   \n",
      "1  N18955     health          medical   \n",
      "2  N61837       news        newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1  Dispose of unwanted prescription drugs during ...   \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1                                                      \n",
      "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                        CombinedText  \n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...  \n",
      "1  Dispose of unwanted prescription drugs during ...  \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...  \n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Preprocess News Data\n",
    "\n",
    "# Ensure pandas is available (should be from previous cells)\n",
    "# import pandas as pd\n",
    "\n",
    "def preprocess_news_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocesses a news dataframe:\n",
    "    - Fills NaNs in key text columns with empty strings.\n",
    "    - Creates a 'CombinedText' column from 'Title' and 'Abstract'.\n",
    "    \"\"\"\n",
    "    # Fill NaN values\n",
    "    text_cols_to_fill = ['Title', 'Abstract', 'Category', 'SubCategory']\n",
    "    for col in text_cols_to_fill:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('')\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in news dataframe during NaN filling.\")\n",
    "\n",
    "    # Combine Title and Abstract\n",
    "    if 'Title' in df.columns and 'Abstract' in df.columns:\n",
    "        df['CombinedText'] = df['Title'] + \" \" + df['Abstract']\n",
    "    else:\n",
    "        print(\"Warning: 'Title' or 'Abstract' column not found. Cannot create 'CombinedText'.\")\n",
    "        # Initialize 'CombinedText' with empty strings if creation fails, to avoid downstream errors\n",
    "        df['CombinedText'] = ''\n",
    "        if 'Title' in df.columns and 'Abstract' not in df.columns:\n",
    "            print(\"Using only 'Title' for 'CombinedText'.\")\n",
    "            df['CombinedText'] = df['Title']\n",
    "        elif 'Abstract' in df.columns and 'Title' not in df.columns:\n",
    "            print(\"Using only 'Abstract' for 'CombinedText'.\")\n",
    "            df['CombinedText'] = df['Abstract']\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Preprocessing train_news_df...\")\n",
    "train_news_df = preprocess_news_dataframe(train_news_df.copy()) # Use .copy() to avoid SettingWithCopyWarning\n",
    "print(\"Train News Data After Preprocessing:\")\n",
    "train_news_df.info(verbose=False, memory_usage='deep')\n",
    "print(\"\\nTrain News Data Head (first 3 rows with CombinedText):\")\n",
    "print(train_news_df[['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'CombinedText']].head(3))\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nPreprocessing dev_news_df...\")\n",
    "dev_news_df = preprocess_news_dataframe(dev_news_df.copy()) # Use .copy() to avoid SettingWithCopyWarning\n",
    "print(\"Dev News Data After Preprocessing:\")\n",
    "dev_news_df.info(verbose=False, memory_usage='deep')\n",
    "print(\"\\nDev News Data Head (first 3 rows with CombinedText):\")\n",
    "print(dev_news_df[['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'CombinedText']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7899bf3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T03:25:51.042752Z",
     "iopub.status.busy": "2025-05-31T03:25:51.042502Z",
     "iopub.status.idle": "2025-05-31T03:26:27.260524Z",
     "shell.execute_reply": "2025-05-31T03:26:27.259770Z"
    },
    "id": "ugj9O3yoawxf",
    "outputId": "470e1b52-fd7f-45e5-ebb6-25174e6a8bd5",
    "papermill": {
     "duration": 36.222682,
     "end_time": "2025-05-31T03:26:27.261600",
     "exception": false,
     "start_time": "2025-05-31T03:25:51.038918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_behaviors_df...\n",
      "Parsing impressions for train_behaviors_df...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train behaviors: 100%|██████████| 156965/156965 [00:16<00:00, 9309.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Behaviors Data After Parsing and Exploding:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5843444 entries, 0 to 5843443\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Dtype \n",
      "---  ------                ----- \n",
      " 0   UserID                object\n",
      " 1   Time                  object\n",
      " 2   History               object\n",
      " 3   ImpressionNewsID      object\n",
      " 4   ClickLabel            int64 \n",
      " 5   OriginalImpressionID  int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 3.3 GB\n",
      "\n",
      "Train Behaviors Data Head (first 5 rows):\n",
      "   UserID                   Time  \\\n",
      "0  U13740  11/11/2019 9:05:58 AM   \n",
      "1  U13740  11/11/2019 9:05:58 AM   \n",
      "2  U91836  11/12/2019 6:11:30 PM   \n",
      "3  U91836  11/12/2019 6:11:30 PM   \n",
      "4  U91836  11/12/2019 6:11:30 PM   \n",
      "\n",
      "                                             History ImpressionNewsID  \\\n",
      "0  [N55189, N42782, N34694, N45794, N18445, N6330...           N55689   \n",
      "1  [N55189, N42782, N34694, N45794, N18445, N6330...           N35729   \n",
      "2  [N31739, N6072, N63045, N23979, N35656, N43353...           N20678   \n",
      "3  [N31739, N6072, N63045, N23979, N35656, N43353...           N39317   \n",
      "4  [N31739, N6072, N63045, N23979, N35656, N43353...           N58114   \n",
      "\n",
      "   ClickLabel  OriginalImpressionID  \n",
      "0           1                     1  \n",
      "1           0                     1  \n",
      "2           0                     2  \n",
      "3           0                     2  \n",
      "4           0                     2  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Processing dev_behaviors_df...\n",
      "Parsing impressions for dev_behaviors_df...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dev behaviors: 100%|██████████| 73152/73152 [00:07<00:00, 9723.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev Behaviors Data After Parsing and Exploding:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2740998 entries, 0 to 2740997\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Dtype \n",
      "---  ------                ----- \n",
      " 0   UserID                object\n",
      " 1   Time                  object\n",
      " 2   History               object\n",
      " 3   ImpressionNewsID      object\n",
      " 4   ClickLabel            int64 \n",
      " 5   OriginalImpressionID  int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.6 GB\n",
      "\n",
      "Dev Behaviors Data Head (first 5 rows):\n",
      "   UserID                    Time  \\\n",
      "0  U80234  11/15/2019 12:37:50 PM   \n",
      "1  U80234  11/15/2019 12:37:50 PM   \n",
      "2  U80234  11/15/2019 12:37:50 PM   \n",
      "3  U80234  11/15/2019 12:37:50 PM   \n",
      "4  U80234  11/15/2019 12:37:50 PM   \n",
      "\n",
      "                                             History ImpressionNewsID  \\\n",
      "0  [N55189, N46039, N51741, N53234, N11276, N264,...           N28682   \n",
      "1  [N55189, N46039, N51741, N53234, N11276, N264,...           N48740   \n",
      "2  [N55189, N46039, N51741, N53234, N11276, N264,...           N31958   \n",
      "3  [N55189, N46039, N51741, N53234, N11276, N264,...           N34130   \n",
      "4  [N55189, N46039, N51741, N53234, N11276, N264,...            N6916   \n",
      "\n",
      "   ClickLabel  OriginalImpressionID  \n",
      "0           0                     1  \n",
      "1           0                     1  \n",
      "2           1                     1  \n",
      "3           0                     1  \n",
      "4           0                     1  \n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Preprocess Behaviors Data - Parse Impressions and History\n",
    "\n",
    "# Ensure pandas is available\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas() # For progress_apply\n",
    "\n",
    "def parse_and_explode_behaviors(df, name=\"train\"):\n",
    "    \"\"\"\n",
    "    Parses 'Impressions' and 'History' columns in a behaviors dataframe.\n",
    "    - 'Impressions' are split into individual (NewsID, ClickLabel) pairs,\n",
    "      exploding the dataframe.\n",
    "    - 'History' is converted from a space-separated string to a list of NewsIDs.\n",
    "    \"\"\"\n",
    "    # Handle NaN in History: replace with empty string for consistent splitting\n",
    "    df['History'] = df['History'].fillna('')\n",
    "    # Split History string into a list of NewsIDs\n",
    "    df['History'] = df['History'].str.split()\n",
    "\n",
    "    # Parse impressions\n",
    "    # Each impression string is like \"N123-1 N456-0...\"\n",
    "    # We want to transform this into new rows: (UserID, Time, History, ImpressionNewsID, ClickLabel)\n",
    "\n",
    "    impression_records = []\n",
    "    print(f\"Parsing impressions for {name}_behaviors_df...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {name} behaviors\"):\n",
    "        user_id = row['UserID']\n",
    "        time = row['Time']\n",
    "        history = row['History'] # Already a list\n",
    "        impression_id_main = row['ImpressionID'] # Keep original impression ID for reference if needed\n",
    "\n",
    "        if pd.isna(row['Impressions']): # Handle cases where 'Impressions' might be NaN\n",
    "            continue\n",
    "\n",
    "        impressions_list = row['Impressions'].split()\n",
    "        for impression_item in impressions_list:\n",
    "            parts = impression_item.split('-')\n",
    "            if len(parts) == 2:\n",
    "                news_id = parts[0]\n",
    "                label = int(parts[1])\n",
    "                impression_records.append({\n",
    "                    'UserID': user_id,\n",
    "                    'Time': time,\n",
    "                    'History': history,\n",
    "                    'ImpressionNewsID': news_id,\n",
    "                    'ClickLabel': label,\n",
    "                    'OriginalImpressionID': impression_id_main # Optional: for tracing back\n",
    "                })\n",
    "\n",
    "    parsed_df = pd.DataFrame(impression_records)\n",
    "    return parsed_df\n",
    "\n",
    "# Process train_behaviors_df\n",
    "# Note: This operation can be memory-intensive and time-consuming for large datasets.\n",
    "# The original train_behaviors_df has 156,965 rows. After exploding, it will be much larger.\n",
    "print(\"Processing train_behaviors_df...\")\n",
    "train_behaviors_parsed_df = parse_and_explode_behaviors(train_behaviors_df.copy(), name=\"train\")\n",
    "print(\"\\nTrain Behaviors Data After Parsing and Exploding:\")\n",
    "train_behaviors_parsed_df.info(memory_usage='deep')\n",
    "print(\"\\nTrain Behaviors Data Head (first 5 rows):\")\n",
    "print(train_behaviors_parsed_df.head())\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Process dev_behaviors_df\n",
    "# The original dev_behaviors_df has 73,152 rows.\n",
    "print(\"\\nProcessing dev_behaviors_df...\")\n",
    "dev_behaviors_parsed_df = parse_and_explode_behaviors(dev_behaviors_df.copy(), name=\"dev\")\n",
    "print(\"\\nDev Behaviors Data After Parsing and Exploding:\")\n",
    "dev_behaviors_parsed_df.info(memory_usage='deep')\n",
    "print(\"\\nDev Behaviors Data Head (first 5 rows):\")\n",
    "print(dev_behaviors_parsed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afba176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T03:26:27.279675Z",
     "iopub.status.busy": "2025-05-31T03:26:27.279478Z",
     "iopub.status.idle": "2025-05-31T03:26:33.942079Z",
     "shell.execute_reply": "2025-05-31T03:26:33.941091Z"
    },
    "id": "LPtF0b0Ma1Gy",
    "outputId": "f5a1c90e-3c40-4730-b964-57036f97aef1",
    "papermill": {
     "duration": 6.673059,
     "end_time": "2025-05-31T03:26:33.943489",
     "exception": false,
     "start_time": "2025-05-31T03:26:27.270430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging training behaviors with news data...\n",
      "\n",
      "Train Merged Data Info:\n",
      "Shape of train_merged_df: (5843444, 15)\n",
      "\n",
      "Train Merged Data Head (first 5 rows):\n",
      "   UserID ImpressionNewsID  ClickLabel Category   SubCategory  \\\n",
      "0  U13740           N55689           1   sports  football_nfl   \n",
      "1  U13740           N35729           0     news        newsus   \n",
      "2  U91836           N20678           0   sports   more_sports   \n",
      "3  U91836           N39317           0     news  newspolitics   \n",
      "4  U91836           N58114           0    autos     autosnews   \n",
      "\n",
      "                                        CombinedText  \\\n",
      "0  Charles Rogers, former Michigan State football...   \n",
      "1  Porsche launches into second story of New Jers...   \n",
      "2  Bode Miller delivered his twin boys after midw...   \n",
      "3  Senior Trump official embellished résumé, had ...   \n",
      "4  2020 Ford Explorer launch hardly went accordin...   \n",
      "\n",
      "                                             History  \n",
      "0  [N55189, N42782, N34694, N45794, N18445, N6330...  \n",
      "1  [N55189, N42782, N34694, N45794, N18445, N6330...  \n",
      "2  [N31739, N6072, N63045, N23979, N35656, N43353...  \n",
      "3  [N31739, N6072, N63045, N23979, N35656, N43353...  \n",
      "4  [N31739, N6072, N63045, N23979, N35656, N43353...  \n",
      "\n",
      "All impressions in training data successfully merged with news details.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Merging development behaviors with news data...\n",
      "\n",
      "Dev Merged Data Info:\n",
      "Shape of dev_merged_df: (2740998, 15)\n",
      "\n",
      "Dev Merged Data Head (first 5 rows):\n",
      "   UserID ImpressionNewsID  ClickLabel       Category   SubCategory  \\\n",
      "0  U80234           N28682           0         sports  football_nfl   \n",
      "1  U80234           N48740           0      lifestyle        voices   \n",
      "2  U80234           N31958           1         sports  football_nfl   \n",
      "3  U80234           N34130           0             tv  tv-celebrity   \n",
      "4  U80234            N6916           0  entertainment     celebrity   \n",
      "\n",
      "                                        CombinedText  \\\n",
      "0  Browns apologize to Mason Rudolph, call Myles ...   \n",
      "1  I've been writing about tiny homes for a year ...   \n",
      "2  Opinion: Colin Kaepernick is about to get what...   \n",
      "3  The Kardashians Face Backlash Over 'Insensitiv...   \n",
      "4  THEN AND NOW: What all your favorite '90s star...   \n",
      "\n",
      "                                             History  \n",
      "0  [N55189, N46039, N51741, N53234, N11276, N264,...  \n",
      "1  [N55189, N46039, N51741, N53234, N11276, N264,...  \n",
      "2  [N55189, N46039, N51741, N53234, N11276, N264,...  \n",
      "3  [N55189, N46039, N51741, N53234, N11276, N264,...  \n",
      "4  [N55189, N46039, N51741, N53234, N11276, N264,...  \n",
      "\n",
      "All impressions in development data successfully merged with news details.\n",
      "\n",
      "Dropped redundant 'NewsID' column after merge if it existed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Merge News Content with Parsed Behaviors Data\n",
    "\n",
    "# Ensure pandas is available\n",
    "# import pandas as pd\n",
    "\n",
    "# Merge training data\n",
    "print(\"Merging training behaviors with news data...\")\n",
    "# We need to merge on the news ID presented in the impression\n",
    "# train_behaviors_parsed_df has 'ImpressionNewsID'\n",
    "# train_news_df has 'NewsID' and its content like 'Category', 'SubCategory', 'CombinedText'\n",
    "train_merged_df = pd.merge(\n",
    "    train_behaviors_parsed_df,\n",
    "    train_news_df,\n",
    "    left_on='ImpressionNewsID',\n",
    "    right_on='NewsID',\n",
    "    how='left' # Use left merge to keep all impressions, even if some news details are missing (though unlikely with MIND)\n",
    ")\n",
    "\n",
    "print(\"\\nTrain Merged Data Info:\")\n",
    "# Displaying info for a potentially very large dataframe. Can be verbose.\n",
    "# Let's check shape and head instead for brevity in output.\n",
    "print(f\"Shape of train_merged_df: {train_merged_df.shape}\")\n",
    "# train_merged_df.info(memory_usage='deep') # This can be slow for very large df\n",
    "\n",
    "print(\"\\nTrain Merged Data Head (first 5 rows):\")\n",
    "# Display relevant columns\n",
    "cols_to_show = ['UserID', 'ImpressionNewsID', 'ClickLabel', 'Category', 'SubCategory', 'CombinedText', 'History']\n",
    "# Check if all expected columns exist after merge before trying to print them\n",
    "existing_cols_to_show_train = [col for col in cols_to_show if col in train_merged_df.columns]\n",
    "print(train_merged_df[existing_cols_to_show_train].head())\n",
    "\n",
    "\n",
    "# Check for any rows where the merge might have failed (i.e., ImpressionNewsID not found in news_df)\n",
    "# This would result in NaN values in columns from news_df\n",
    "missing_news_info_train = train_merged_df['NewsID'].isnull().sum()\n",
    "if missing_news_info_train > 0:\n",
    "    print(f\"\\nWarning: {missing_news_info_train} impressions in training data did not have matching news details.\")\n",
    "else:\n",
    "    print(\"\\nAll impressions in training data successfully merged with news details.\")\n",
    "\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Merge development data\n",
    "print(\"\\nMerging development behaviors with news data...\")\n",
    "dev_merged_df = pd.merge(\n",
    "    dev_behaviors_parsed_df,\n",
    "    dev_news_df,\n",
    "    left_on='ImpressionNewsID',\n",
    "    right_on='NewsID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\nDev Merged Data Info:\")\n",
    "print(f\"Shape of dev_merged_df: {dev_merged_df.shape}\")\n",
    "# dev_merged_df.info(memory_usage='deep')\n",
    "\n",
    "print(\"\\nDev Merged Data Head (first 5 rows):\")\n",
    "existing_cols_to_show_dev = [col for col in cols_to_show if col in dev_merged_df.columns]\n",
    "print(dev_merged_df[existing_cols_to_show_dev].head())\n",
    "\n",
    "missing_news_info_dev = dev_merged_df['NewsID'].isnull().sum()\n",
    "if missing_news_info_dev > 0:\n",
    "    print(f\"\\nWarning: {missing_news_info_dev} impressions in development data did not have matching news details.\")\n",
    "else:\n",
    "    print(\"\\nAll impressions in development data successfully merged with news details.\")\n",
    "\n",
    "# We can drop the redundant NewsID column now, as ImpressionNewsID serves the purpose\n",
    "if 'NewsID' in train_merged_df.columns:\n",
    "    train_merged_df = train_merged_df.drop(columns=['NewsID'])\n",
    "if 'NewsID' in dev_merged_df.columns:\n",
    "    dev_merged_df = dev_merged_df.drop(columns=['NewsID'])\n",
    "\n",
    "print(\"\\nDropped redundant 'NewsID' column after merge if it existed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e383db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T03:26:33.962728Z",
     "iopub.status.busy": "2025-05-31T03:26:33.962493Z",
     "iopub.status.idle": "2025-05-31T03:26:42.533245Z",
     "shell.execute_reply": "2025-05-31T03:26:42.532596Z"
    },
    "id": "Rwl9c5EBa92D",
    "papermill": {
     "duration": 8.580883,
     "end_time": "2025-05-31T03:26:42.534241",
     "exception": false,
     "start_time": "2025-05-31T03:26:33.953358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing all unique news articles for TF-IDF processing.\n",
      "Total unique news articles for TF-IDF processing: 65238\n",
      "Sample of unique news articles dataframe:\n",
      "   NewsID                                       CombinedText\n",
      "0  N55528  The Brands Queen Elizabeth, Prince Charles, an...\n",
      "1  N19639  50 Worst Habits For Belly Fat These seemingly ...\n",
      "2  N61837  The Cost of Trump's Aid Freeze in the Trenches...\n",
      "3  N53526  I Was An NBA Wife. Here's How It Affected My M...\n",
      "4  N38324  How to Get Rid of Skin Tags, According to a De...\n",
      "\n",
      "Step 2: Fitting TfidfVectorizer on training news text.\n",
      "Fitting TF-IDF on 51282 training news articles' CombinedText...\n",
      "TfidfVectorizer fitted successfully.\n",
      "\n",
      "Step 3: Transforming CombinedText of all unique news articles.\n",
      "Shape of all_news_tfidf_matrix (num_unique_news, num_tfidf_features): (65238, 10000)\n",
      "\n",
      "Step 4: Creating a mapping from NewsID to its TF-IDF vector index.\n",
      "Created newsid_to_tfidf_idx mapping for 65238 news articles.\n",
      "TF-IDF related objects are ready and optionally saved to /kaggle/working/model_assets/.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Feature Engineering - TF-IDF for News Content\n",
    "\n",
    "# Ensure necessary libraries are available\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pandas as pd\n",
    "# import numpy as np # Will be needed in the next cell for averaging\n",
    "\n",
    "# (Variables: train_news_df, dev_news_df should be available from previous cells)\n",
    "\n",
    "print(\"Step 1: Preparing all unique news articles for TF-IDF processing.\")\n",
    "# Combine news from train and dev sets to get a comprehensive list of articles\n",
    "# and their 'CombinedText'.\n",
    "# We select only 'NewsID' and 'CombinedText' to keep this dataframe lean.\n",
    "train_news_subset_for_tfidf = train_news_df[['NewsID', 'CombinedText']].copy()\n",
    "dev_news_subset_for_tfidf = dev_news_df[['NewsID', 'CombinedText']].copy()\n",
    "\n",
    "all_news_for_tfidf_df = pd.concat(\n",
    "    [train_news_subset_for_tfidf, dev_news_subset_for_tfidf],\n",
    "    ignore_index=True\n",
    ")\n",
    "# Drop duplicate NewsIDs, keeping the first occurrence.\n",
    "all_news_for_tfidf_df = all_news_for_tfidf_df.drop_duplicates(subset=['NewsID']).reset_index(drop=True)\n",
    "\n",
    "# Ensure 'CombinedText' has no NaN values (should be handled by Cell 4, but good to double-check)\n",
    "all_news_for_tfidf_df['CombinedText'] = all_news_for_tfidf_df['CombinedText'].fillna('')\n",
    "\n",
    "print(f\"Total unique news articles for TF-IDF processing: {len(all_news_for_tfidf_df)}\")\n",
    "print(\"Sample of unique news articles dataframe:\")\n",
    "print(all_news_for_tfidf_df.head())\n",
    "\n",
    "print(\"\\nStep 2: Fitting TfidfVectorizer on training news text.\")\n",
    "# Initialize TfidfVectorizer.\n",
    "# max_features limits the vocabulary size. Adjust as needed.\n",
    "# ngram_range=(1, 2) includes unigrams and bigrams.\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # You can tune this parameter\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# Fit the vectorizer ONLY on the 'CombinedText' from the original train_news_df.\n",
    "# This ensures that the vocabulary and IDF weights are learned only from training data.\n",
    "# Make sure train_news_df['CombinedText'] has no NaNs (handled in Cell 4).\n",
    "print(f\"Fitting TF-IDF on {len(train_news_df)} training news articles' CombinedText...\")\n",
    "# If train_news_df has NaNs in 'CombinedText' despite earlier preprocessing, fill them here.\n",
    "# train_news_df['CombinedText'] = train_news_df['CombinedText'].fillna('') # Should not be necessary if Cell 4 ran correctly.\n",
    "tfidf_vectorizer.fit(train_news_df['CombinedText'])\n",
    "print(\"TfidfVectorizer fitted successfully.\")\n",
    "\n",
    "print(\"\\nStep 3: Transforming CombinedText of all unique news articles.\")\n",
    "# Transform the 'CombinedText' of ALL unique news articles using the FITTED vectorizer.\n",
    "all_news_tfidf_matrix = tfidf_vectorizer.transform(all_news_for_tfidf_df['CombinedText'])\n",
    "print(f\"Shape of all_news_tfidf_matrix (num_unique_news, num_tfidf_features): {all_news_tfidf_matrix.shape}\")\n",
    "\n",
    "print(\"\\nStep 4: Creating a mapping from NewsID to its TF-IDF vector index.\")\n",
    "# This dictionary will allow us to quickly find the row in all_news_tfidf_matrix for a given NewsID.\n",
    "newsid_to_tfidf_idx = pd.Series(\n",
    "    all_news_for_tfidf_df.index,\n",
    "    index=all_news_for_tfidf_df['NewsID']\n",
    ").to_dict()\n",
    "print(f\"Created newsid_to_tfidf_idx mapping for {len(newsid_to_tfidf_idx)} news articles.\")\n",
    "\n",
    "# Optional: Save the fitted vectorizer and related objects if you plan to stop and resume.\n",
    "import pickle\n",
    "output_path = '/kaggle/working/model_assets/' # Define your save path\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "with open(os.path.join(output_path, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "with open(os.path.join(output_path, 'all_news_tfidf_matrix.pkl'), 'wb') as f: # Note: saving sparse matrices this way might be inefficient for very large ones. Consider scipy.sparse.save_npz\n",
    "    pickle.dump(all_news_tfidf_matrix, f)\n",
    "with open(os.path.join(output_path, 'newsid_to_tfidf_idx.pkl'), 'wb') as f:\n",
    "    pickle.dump(newsid_to_tfidf_idx, f)\n",
    "print(f\"TF-IDF related objects are ready and optionally saved to {output_path}.\")\n",
    "\n",
    "# Key outputs of this cell (available in memory for the next cell):\n",
    "# - tfidf_vectorizer: The fitted TfidfVectorizer.\n",
    "# - all_news_tfidf_matrix: The sparse matrix of TF-IDF features for all unique news.\n",
    "# - newsid_to_tfidf_idx: Dictionary mapping NewsID to its row index in all_news_tfidf_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3029aecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T03:26:42.552577Z",
     "iopub.status.busy": "2025-05-31T03:26:42.552379Z",
     "iopub.status.idle": "2025-05-31T04:30:44.762409Z",
     "shell.execute_reply": "2025-05-31T04:30:44.761410Z"
    },
    "id": "mQcCILEzeuPR",
    "papermill": {
     "duration": 3842.220666,
     "end_time": "2025-05-31T04:30:44.763675",
     "exception": false,
     "start_time": "2025-05-31T03:26:42.543009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 & 2 Combined: Calculating Interaction Features (Cosine Similarity)...\n",
      "Calculating cosine similarities for Train Interactions data (this might take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Interactions: 100%|██████████| 5843444/5843444 [42:50<00:00, 2273.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity feature added to train_merged_df.\n",
      "Calculating cosine similarities for Dev Interactions data (this might take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Interactions: 100%|██████████| 2740998/2740998 [21:02<00:00, 2170.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity feature added to dev_merged_df.\n",
      "\n",
      "Sample of train_merged_df with new cosine similarity feature:\n",
      "   UserID ImpressionNewsID  ClickLabel  HistoryImpressionCosineSimilarity\n",
      "0  U13740           N55689           1                           0.033206\n",
      "1  U13740           N35729           0                           0.010533\n",
      "2  U91836           N20678           0                           0.016130\n",
      "3  U91836           N39317           0                           0.028578\n",
      "4  U91836           N58114           0                           0.022575\n",
      "\n",
      "Step 3: Label Encoding UserID and ImpressionNewsID...\n",
      "Fitting LabelEncoders on combined training and development data identifiers...\n",
      "Transforming UserID and ImpressionNewsID...\n",
      "UserID and ImpressionNewsID encoded.\n",
      "Sample of train_merged_df with encoded IDs:\n",
      "   UserID  UserID_Encoded ImpressionNewsID  ImpressionNewsID_Encoded  \\\n",
      "0  U13740            4158           N55689                     17658   \n",
      "1  U13740            4158           N35729                      9923   \n",
      "2  U91836           90930           N20678                      4119   \n",
      "3  U91836           90930           N39317                     11321   \n",
      "4  U91836           90930           N58114                     18601   \n",
      "\n",
      "   ClickLabel  \n",
      "0           1  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "Label encoders optionally saved to /kaggle/working/model_assets/.\n",
      "\n",
      "Final check of train_merged_df (relevant columns) after feature engineering:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5843444 entries, 0 to 5843443\n",
      "Data columns (total 6 columns):\n",
      " #   Column                             Dtype  \n",
      "---  ------                             -----  \n",
      " 0   UserID                             object \n",
      " 1   ImpressionNewsID                   object \n",
      " 2   ClickLabel                         int64  \n",
      " 3   HistoryImpressionCosineSimilarity  float64\n",
      " 4   UserID_Encoded                     int64  \n",
      " 5   ImpressionNewsID_Encoded           int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 878.9 MB\n",
      "\n",
      "Final check of dev_merged_df (relevant columns) after feature engineering:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2740998 entries, 0 to 2740997\n",
      "Data columns (total 6 columns):\n",
      " #   Column                             Dtype  \n",
      "---  ------                             -----  \n",
      " 0   UserID                             object \n",
      " 1   ImpressionNewsID                   object \n",
      " 2   ClickLabel                         int64  \n",
      " 3   HistoryImpressionCosineSimilarity  float64\n",
      " 4   UserID_Encoded                     int64  \n",
      " 5   ImpressionNewsID_Encoded           int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 412.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Feature Engineering - User History Profile & Interaction Features (Memory Optimized)\n",
    "\n",
    "# Ensure necessary libraries/objects are available\n",
    "from sklearn.metrics.pairwise import cosine_similarity # <<<< ADD THIS IMPORT\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse # For type checking or operations if needed, though main use is via tfidf_matrix\n",
    "# tqdm.pandas() # For df.progress_apply if used, but we iterate here.\n",
    "\n",
    "# Objects from previous cell (ensure Cell 7 was run successfully in this session):\n",
    "# - all_news_tfidf_matrix (sparse matrix of TF-IDF features for all news)\n",
    "# - newsid_to_tfidf_idx (dict mapping NewsID to its row index in all_news_tfidf_matrix)\n",
    "# DataFrames (ensure Cells 1-6 were run successfully in this session):\n",
    "# - train_merged_df, dev_merged_df\n",
    "\n",
    "# Helper function to get TF-IDF vector for a news_id (as a dense 1D float32 array)\n",
    "def get_dense_tfidf_vector_float32(news_id, newsid_to_idx_map, tfidf_matrix, num_features):\n",
    "    if news_id in newsid_to_idx_map:\n",
    "        idx = newsid_to_idx_map[news_id]\n",
    "        # Convert sparse row vector to dense 1D np.ndarray with float32 dtype\n",
    "        return np.asarray(tfidf_matrix[idx].todense(), dtype=np.float32).flatten()\n",
    "    return np.zeros(num_features, dtype=np.float32) # Return zero vector if not found\n",
    "\n",
    "\n",
    "print(\"Step 1 & 2 Combined: Calculating Interaction Features (Cosine Similarity)...\")\n",
    "\n",
    "# Cache for user history profiles: {frozenset(history_list) -> 1D_dense_profile_vector_float32}\n",
    "history_profile_cache = {}\n",
    "# Ensure all_news_tfidf_matrix is defined by running Cell 7\n",
    "num_tfidf_features = all_news_tfidf_matrix.shape[1]\n",
    "\n",
    "def calculate_interaction_features_for_df(df, newsid_to_idx_map, tfidf_matrix, desc=\"Processing\"):\n",
    "    cosine_similarities_list = [] # Renamed to avoid conflict if cosine_similarity was a variable\n",
    "\n",
    "    print(f\"Calculating cosine similarities for {desc} data (this might take a while)...\")\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        history_list = row['History'] # List of NewsIDs\n",
    "        impression_news_id = row['ImpressionNewsID']\n",
    "\n",
    "        # --- Get/Compute User History Profile Vector ---\n",
    "        current_history_key = frozenset(history_list if isinstance(history_list, list) else [])\n",
    "\n",
    "        if current_history_key in history_profile_cache:\n",
    "            user_profile_vector = history_profile_cache[current_history_key]\n",
    "        else:\n",
    "            history_item_vectors = []\n",
    "            if isinstance(history_list, list) and len(history_list) > 0:\n",
    "                for news_id_in_history in history_list:\n",
    "                    vec = get_dense_tfidf_vector_float32(news_id_in_history, newsid_to_idx_map, tfidf_matrix, num_tfidf_features)\n",
    "                    if np.any(vec):\n",
    "                        history_item_vectors.append(vec)\n",
    "\n",
    "            if history_item_vectors:\n",
    "                user_profile_vector = np.mean(np.array(history_item_vectors, dtype=np.float32), axis=0)\n",
    "            else:\n",
    "                user_profile_vector = np.zeros(num_tfidf_features, dtype=np.float32)\n",
    "            history_profile_cache[current_history_key] = user_profile_vector\n",
    "\n",
    "        # --- Get Impression News Vector ---\n",
    "        impression_news_vector = get_dense_tfidf_vector_float32(impression_news_id, newsid_to_idx_map, tfidf_matrix, num_tfidf_features)\n",
    "\n",
    "        # --- Calculate Cosine Similarity ---\n",
    "        u_vec_2d = user_profile_vector.reshape(1, -1)\n",
    "        i_vec_2d = impression_news_vector.reshape(1, -1)\n",
    "\n",
    "        if np.all(u_vec_2d == 0) or np.all(i_vec_2d == 0):\n",
    "            similarity_value = 0.0 # Renamed to avoid conflict\n",
    "        else:\n",
    "            similarity_value = cosine_similarity(u_vec_2d, i_vec_2d)[0, 0] # Uses the imported function\n",
    "\n",
    "        cosine_similarities_list.append(similarity_value)\n",
    "\n",
    "    df['HistoryImpressionCosineSimilarity'] = cosine_similarities_list\n",
    "    return df\n",
    "\n",
    "# Process Training Data\n",
    "train_merged_df = calculate_interaction_features_for_df(\n",
    "    train_merged_df, newsid_to_tfidf_idx, all_news_tfidf_matrix, desc=\"Train Interactions\"\n",
    ")\n",
    "print(\"Cosine similarity feature added to train_merged_df.\")\n",
    "\n",
    "# Process Development Data\n",
    "dev_merged_df = calculate_interaction_features_for_df(\n",
    "    dev_merged_df, newsid_to_tfidf_idx, all_news_tfidf_matrix, desc=\"Dev Interactions\"\n",
    ")\n",
    "print(\"Cosine similarity feature added to dev_merged_df.\")\n",
    "\n",
    "print(\"\\nSample of train_merged_df with new cosine similarity feature:\")\n",
    "if 'HistoryImpressionCosineSimilarity' in train_merged_df.columns:\n",
    "    print(train_merged_df[['UserID', 'ImpressionNewsID', 'ClickLabel', 'HistoryImpressionCosineSimilarity']].head())\n",
    "\n",
    "print(\"\\nStep 3: Label Encoding UserID and ImpressionNewsID...\")\n",
    "user_encoder = LabelEncoder()\n",
    "news_encoder = LabelEncoder()\n",
    "\n",
    "print(\"Fitting LabelEncoders on combined training and development data identifiers...\")\n",
    "combined_user_ids = pd.concat([train_merged_df['UserID'].astype(str), dev_merged_df['UserID'].astype(str)]).unique()\n",
    "combined_news_ids = pd.concat([train_merged_df['ImpressionNewsID'].astype(str), dev_merged_df['ImpressionNewsID'].astype(str)]).unique()\n",
    "\n",
    "user_encoder.fit(combined_user_ids)\n",
    "news_encoder.fit(combined_news_ids)\n",
    "\n",
    "print(\"Transforming UserID and ImpressionNewsID...\")\n",
    "train_merged_df['UserID_Encoded'] = user_encoder.transform(train_merged_df['UserID'].astype(str))\n",
    "train_merged_df['ImpressionNewsID_Encoded'] = news_encoder.transform(train_merged_df['ImpressionNewsID'].astype(str))\n",
    "dev_merged_df['UserID_Encoded'] = user_encoder.transform(dev_merged_df['UserID'].astype(str))\n",
    "dev_merged_df['ImpressionNewsID_Encoded'] = news_encoder.transform(dev_merged_df['ImpressionNewsID'].astype(str))\n",
    "\n",
    "print(\"UserID and ImpressionNewsID encoded.\")\n",
    "print(\"Sample of train_merged_df with encoded IDs:\")\n",
    "if 'UserID_Encoded' in train_merged_df.columns and 'ImpressionNewsID_Encoded' in train_merged_df.columns:\n",
    "    print(train_merged_df[['UserID', 'UserID_Encoded', 'ImpressionNewsID', 'ImpressionNewsID_Encoded', 'ClickLabel']].head())\n",
    "\n",
    "# Optional: Saving encoders (ensure output_path is defined and os is imported if not already)\n",
    "import os # Make sure os is imported\n",
    "output_path = '/kaggle/working/model_assets/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "import pickle # Make sure pickle is imported\n",
    "with open(os.path.join(output_path, 'user_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(user_encoder, f)\n",
    "with open(os.path.join(output_path, 'news_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(news_encoder, f)\n",
    "print(f\"Label encoders optionally saved to {output_path}.\")\n",
    "\n",
    "print(\"\\nFinal check of train_merged_df (relevant columns) after feature engineering:\")\n",
    "cols_to_check = ['UserID', 'ImpressionNewsID', 'ClickLabel', 'HistoryImpressionCosineSimilarity', 'UserID_Encoded', 'ImpressionNewsID_Encoded']\n",
    "existing_cols_train = [col for col in cols_to_check if col in train_merged_df.columns]\n",
    "if existing_cols_train:\n",
    "    train_merged_df[existing_cols_train].info(memory_usage='deep')\n",
    "\n",
    "print(\"\\nFinal check of dev_merged_df (relevant columns) after feature engineering:\")\n",
    "existing_cols_dev = [col for col in cols_to_check if col in dev_merged_df.columns]\n",
    "if existing_cols_dev:\n",
    "    dev_merged_df[existing_cols_dev].info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4897ec2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:30:47.483848Z",
     "iopub.status.busy": "2025-05-31T04:30:47.483606Z",
     "iopub.status.idle": "2025-05-31T04:30:51.340475Z",
     "shell.execute_reply": "2025-05-31T04:30:51.339467Z"
    },
    "papermill": {
     "duration": 5.219294,
     "end_time": "2025-05-31T04:30:51.341735",
     "exception": false,
     "start_time": "2025-05-31T04:30:46.122441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.0rc1)\r\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\r\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\r\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "574dc8f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:30:54.047514Z",
     "iopub.status.busy": "2025-05-31T04:30:54.047246Z",
     "iopub.status.idle": "2025-05-31T04:32:02.346499Z",
     "shell.execute_reply": "2025-05-31T04:32:02.345924Z"
    },
    "id": "uhb6tVt6egRc",
    "papermill": {
     "duration": 69.647244,
     "end_time": "2025-05-31T04:32:02.350227",
     "exception": false,
     "start_time": "2025-05-31T04:30:52.702983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for LightGBM model...\n",
      "Training data shape: X_train (5843444, 3), y_train (5843444,)\n",
      "Development data shape: X_dev (2740998, 3), y_dev (2740998,)\n",
      "\n",
      "Training LightGBM model...\n",
      "\n",
      "Training target class distribution:\n",
      "ClickLabel\n",
      "0    0.959554\n",
      "1    0.040446\n",
      "Name: proportion, dtype: float64\n",
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's auc: 0.564681\n",
      "\n",
      "Model training completed.\n",
      "\n",
      "Evaluating model on the development set...\n",
      "Development Set AUC: 0.5647\n",
      "Development Set Accuracy: 0.9594\n",
      "\n",
      "Development Set Confusion Matrix:\n",
      "[[2629615       0]\n",
      " [ 111383       0]]\n",
      "\n",
      "Development Set Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98   2629615\n",
      "           1       0.00      0.00      0.00    111383\n",
      "\n",
      "    accuracy                           0.96   2740998\n",
      "   macro avg       0.48      0.50      0.49   2740998\n",
      "weighted avg       0.92      0.96      0.94   2740998\n",
      "\n",
      "\n",
      "Feature Importances:\n",
      "                             feature  importance\n",
      "1           ImpressionNewsID_Encoded        1592\n",
      "2  HistoryImpressionCosineSimilarity         208\n",
      "0                     UserID_Encoded           0\n",
      "\n",
      "Trained LightGBM model saved to /kaggle/working/model_assets/lgbm_news_recommender_v1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Prepare Data for LightGBM and Train Model\n",
    "\n",
    "# Ensure necessary libraries are available\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "# import pandas as pd # Should be available\n",
    "# import numpy as np # Should be available\n",
    "\n",
    "# DataFrames from previous cell: train_merged_df, dev_merged_df\n",
    "# These should have:\n",
    "# 'UserID_Encoded', 'ImpressionNewsID_Encoded', 'HistoryImpressionCosineSimilarity', 'ClickLabel'\n",
    "\n",
    "print(\"Preparing data for LightGBM model...\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# We can add more features here if we engineered them (e.g., encoded categories, time features, etc.)\n",
    "# For now, focusing on the core ones.\n",
    "feature_columns = ['UserID_Encoded', 'ImpressionNewsID_Encoded', 'HistoryImpressionCosineSimilarity']\n",
    "target_column = 'ClickLabel'\n",
    "\n",
    "# Training data\n",
    "X_train = train_merged_df[feature_columns]\n",
    "y_train = train_merged_df[target_column]\n",
    "\n",
    "# Development (validation) data\n",
    "X_dev = dev_merged_df[feature_columns]\n",
    "y_dev = dev_merged_df[target_column]\n",
    "\n",
    "print(f\"Training data shape: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Development data shape: X_dev {X_dev.shape}, y_dev {y_dev.shape}\")\n",
    "\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "\n",
    "# Initialize LGBMClassifier\n",
    "# These are some basic parameters; extensive hyperparameter tuning can improve performance.\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    objective='binary',        # Binary classification\n",
    "    metric='auc',              # Evaluation metric: Area Under ROC Curve\n",
    "    n_estimators=1000,         # Number of boosting rounds (trees)\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,              # No limit on tree depth\n",
    "    random_state=42,           # For reproducibility\n",
    "    n_jobs=-1,                 # Use all available cores\n",
    "    colsample_bytree=0.8,      # Subsample ratio of columns when constructing each tree\n",
    "    subsample=0.8,             # Subsample ratio of the training instance\n",
    "    reg_alpha=0.1,             # L1 regularization\n",
    "    reg_lambda=0.1             # L2 regularization\n",
    "    # class_weight='balanced' # Useful if classes are imbalanced, check y_train.value_counts()\n",
    ")\n",
    "\n",
    "# Check class balance for training data\n",
    "print(\"\\nTraining target class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "# If highly imbalanced, consider using scale_pos_weight or class_weight='balanced'\n",
    "\n",
    "# Train the model\n",
    "# Using X_dev, y_dev as an evaluation set for early stopping\n",
    "lgbm_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_dev, y_dev)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(100, verbose=True)] # Updated: lgb.early_stopping\n",
    "                                                      # Early stopping: stops if 'auc' on eval set doesn't improve for 100 rounds.\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed.\")\n",
    "\n",
    "print(\"\\nEvaluating model on the development set...\")\n",
    "# Make predictions (probabilities for the positive class)\n",
    "y_pred_proba_dev = lgbm_model.predict_proba(X_dev)[:, 1]\n",
    "\n",
    "# Make predictions (class labels) using a default threshold of 0.5\n",
    "y_pred_class_dev = lgbm_model.predict(X_dev) # (thresholds 0.5 by default)\n",
    "# Or, from probabilities: y_pred_class_dev = (y_pred_proba_dev >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_dev, y_pred_proba_dev)\n",
    "print(f\"Development Set AUC: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_dev, y_pred_class_dev)\n",
    "print(f\"Development Set Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nDevelopment Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_dev, y_pred_class_dev))\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDevelopment Set Classification Report:\")\n",
    "print(classification_report(y_dev, y_pred_class_dev))\n",
    "\n",
    "\n",
    "# Feature Importance (optional, but insightful)\n",
    "print(\"\\nFeature Importances:\")\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': lgbm_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "\n",
    "# Optional: Save the trained model\n",
    "import pickle\n",
    "output_path = '/kaggle/working/model_assets/' # Should be defined\n",
    "os.makedirs(output_path, exist_ok=True) # Ensure os is imported and path exists\n",
    "model_filename = os.path.join(output_path, 'lgbm_news_recommender_v1.pkl')\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(lgbm_model, f)\n",
    "print(f\"\\nTrained LightGBM model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e0cd45f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:32:05.096726Z",
     "iopub.status.busy": "2025-05-31T04:32:05.096213Z",
     "iopub.status.idle": "2025-05-31T04:32:07.808303Z",
     "shell.execute_reply": "2025-05-31T04:32:07.807103Z"
    },
    "id": "kumEki8Sfhvs",
    "papermill": {
     "duration": 4.092221,
     "end_time": "2025-05-31T04:32:07.810360",
     "exception": false,
     "start_time": "2025-05-31T04:32:03.718139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install optuna -q # -q for quiet installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fbbd87e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:32:10.563129Z",
     "iopub.status.busy": "2025-05-31T04:32:10.562364Z",
     "iopub.status.idle": "2025-05-31T04:57:26.834191Z",
     "shell.execute_reply": "2025-05-31T04:57:26.833099Z"
    },
    "id": "OWLN5JXUfO61",
    "papermill": {
     "duration": 1517.643117,
     "end_time": "2025-05-31T04:57:26.835709",
     "exception": false,
     "start_time": "2025-05-31T04:32:09.192592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:32:10,570] A new study created in memory with name: no-name-c4d2cb88-b78d-4824-bc05-0143d7162303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna hyperparameter search...\n",
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:32:54,093] Trial 0 finished with value: 0.5606485774818724 and parameters: {'n_estimators': 1202, 'learning_rate': 0.013606514446895583, 'num_leaves': 30, 'max_depth': 11, 'min_child_samples': 38, 'subsample': 0.7815576667070994, 'colsample_bytree': 0.9233677735522392, 'reg_alpha': 0.03544739523462459, 'reg_lambda': 0.47469757725140127}. Best is trial 0 with value: 0.5606485774818724.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130522 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:33:21,515] Trial 1 finished with value: 0.5708285947652062 and parameters: {'n_estimators': 613, 'learning_rate': 0.010119937501293473, 'num_leaves': 142, 'max_depth': 13, 'min_child_samples': 25, 'subsample': 0.7727776069827235, 'colsample_bytree': 0.9993936099118903, 'reg_alpha': 0.18118786451205504, 'reg_lambda': 0.32601530068657425}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:33:43,652] Trial 2 finished with value: 0.5615991323911774 and parameters: {'n_estimators': 1256, 'learning_rate': 0.015245105780278628, 'num_leaves': 149, 'max_depth': 6, 'min_child_samples': 5, 'subsample': 0.6453037251515238, 'colsample_bytree': 0.9826566123884832, 'reg_alpha': 0.9968917396040292, 'reg_lambda': 0.7226351280555013}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:34:02,845] Trial 3 finished with value: 0.55215230426567 and parameters: {'n_estimators': 586, 'learning_rate': 0.01907698108748108, 'num_leaves': 101, 'max_depth': 4, 'min_child_samples': 73, 'subsample': 0.8218379023009434, 'colsample_bytree': 0.5884593496371148, 'reg_alpha': 0.11876375557503005, 'reg_lambda': 0.18773301608098403}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:34:54,046] Trial 4 finished with value: 0.5638156467501971 and parameters: {'n_estimators': 574, 'learning_rate': 0.06172545771744644, 'num_leaves': 148, 'max_depth': 5, 'min_child_samples': 95, 'subsample': 0.8283436424155326, 'colsample_bytree': 0.7345914733419556, 'reg_alpha': 0.8654071156376416, 'reg_lambda': 0.22752739733658767}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:36:09,047] Trial 5 finished with value: 0.5611365506739792 and parameters: {'n_estimators': 220, 'learning_rate': 0.012801550044155247, 'num_leaves': 22, 'max_depth': 7, 'min_child_samples': 18, 'subsample': 0.6899618816279205, 'colsample_bytree': 0.9768168425856516, 'reg_alpha': 0.06512452674246849, 'reg_lambda': 0.755741981797773}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:36:40,593] Trial 6 finished with value: 0.5697405557047437 and parameters: {'n_estimators': 608, 'learning_rate': 0.03095326237295062, 'num_leaves': 120, 'max_depth': 14, 'min_child_samples': 98, 'subsample': 0.9723740182214704, 'colsample_bytree': 0.9177891547877436, 'reg_alpha': 0.5645607016192047, 'reg_lambda': 0.17462419569973786}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:37:20,233] Trial 7 finished with value: 0.5600620057410868 and parameters: {'n_estimators': 470, 'learning_rate': 0.011442361962990633, 'num_leaves': 100, 'max_depth': 3, 'min_child_samples': 6, 'subsample': 0.9890579692052897, 'colsample_bytree': 0.832073074623997, 'reg_alpha': 0.3063109933901197, 'reg_lambda': 0.8639201395542101}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:37:37,558] Trial 8 finished with value: 0.5508362550186705 and parameters: {'n_estimators': 1919, 'learning_rate': 0.04137693444593425, 'num_leaves': 97, 'max_depth': 4, 'min_child_samples': 38, 'subsample': 0.7813019569089403, 'colsample_bytree': 0.6402557460199261, 'reg_alpha': 0.7432361812003483, 'reg_lambda': 0.3364727523829222}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:37:53,293] Trial 9 finished with value: 0.5565302713349217 and parameters: {'n_estimators': 1876, 'learning_rate': 0.04292706824139983, 'num_leaves': 111, 'max_depth': 3, 'min_child_samples': 51, 'subsample': 0.8284292070782225, 'colsample_bytree': 0.9704029358625821, 'reg_alpha': 0.3448577801453415, 'reg_lambda': 0.5009018855729084}. Best is trial 1 with value: 0.5708285947652062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:38:18,500] Trial 10 finished with value: 0.5781267851383756 and parameters: {'n_estimators': 924, 'learning_rate': 0.09687431790563442, 'num_leaves': 65, 'max_depth': 15, 'min_child_samples': 28, 'subsample': 0.6997828100101848, 'colsample_bytree': 0.42513478688623474, 'reg_alpha': 0.3305320605086387, 'reg_lambda': 0.516601781053087}. Best is trial 10 with value: 0.5781267851383756.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:38:44,144] Trial 11 finished with value: 0.5780828588660789 and parameters: {'n_estimators': 914, 'learning_rate': 0.09915635960817784, 'num_leaves': 62, 'max_depth': 14, 'min_child_samples': 27, 'subsample': 0.7164093069125737, 'colsample_bytree': 0.4028904156372342, 'reg_alpha': 0.31175918404379754, 'reg_lambda': 0.5338153940420849}. Best is trial 10 with value: 0.5781267851383756.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:39:09,797] Trial 12 finished with value: 0.5789410301951485 and parameters: {'n_estimators': 957, 'learning_rate': 0.09035257446504434, 'num_leaves': 61, 'max_depth': 15, 'min_child_samples': 30, 'subsample': 0.7012974416579587, 'colsample_bytree': 0.40894546500107765, 'reg_alpha': 0.4748034675035624, 'reg_lambda': 0.580884129120939}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:39:35,553] Trial 13 finished with value: 0.5782082446554758 and parameters: {'n_estimators': 951, 'learning_rate': 0.08650718493956142, 'num_leaves': 64, 'max_depth': 11, 'min_child_samples': 59, 'subsample': 0.6268801894851949, 'colsample_bytree': 0.41842665474984947, 'reg_alpha': 0.5391482664461406, 'reg_lambda': 0.6347494600787215}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:41:12,297] Trial 14 finished with value: 0.5650643122217078 and parameters: {'n_estimators': 1555, 'learning_rate': 0.0675578408596712, 'num_leaves': 62, 'max_depth': 11, 'min_child_samples': 64, 'subsample': 0.6116886662132359, 'colsample_bytree': 0.5050845027114972, 'reg_alpha': 0.5551610911648762, 'reg_lambda': 0.016340963868375136}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:42:23,689] Trial 15 finished with value: 0.5668778711027129 and parameters: {'n_estimators': 959, 'learning_rate': 0.06815697417841901, 'num_leaves': 45, 'max_depth': 9, 'min_child_samples': 56, 'subsample': 0.6073940440617054, 'colsample_bytree': 0.5038630298433896, 'reg_alpha': 0.6758089966084253, 'reg_lambda': 0.9856339816617755}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:42:49,840] Trial 16 finished with value: 0.5782052035731708 and parameters: {'n_estimators': 1411, 'learning_rate': 0.024533145286420664, 'num_leaves': 78, 'max_depth': 11, 'min_child_samples': 45, 'subsample': 0.6537346990554709, 'colsample_bytree': 0.4874097452772259, 'reg_alpha': 0.48279118008636857, 'reg_lambda': 0.6842761140198615}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:44:24,172] Trial 17 finished with value: 0.5656077065197896 and parameters: {'n_estimators': 813, 'learning_rate': 0.051300470393081894, 'num_leaves': 42, 'max_depth': 8, 'min_child_samples': 78, 'subsample': 0.9051926812648186, 'colsample_bytree': 0.6059493193997275, 'reg_alpha': 0.45402124630591134, 'reg_lambda': 0.655747802592471}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:44:46,459] Trial 18 finished with value: 0.5654508638989793 and parameters: {'n_estimators': 1555, 'learning_rate': 0.08257100567313497, 'num_leaves': 80, 'max_depth': 12, 'min_child_samples': 64, 'subsample': 0.7502835291411981, 'colsample_bytree': 0.7004526680003816, 'reg_alpha': 0.6766343665205092, 'reg_lambda': 0.8388721677824922}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:45:09,100] Trial 19 finished with value: 0.5781225033819892 and parameters: {'n_estimators': 1092, 'learning_rate': 0.05298305094822387, 'num_leaves': 47, 'max_depth': 15, 'min_child_samples': 84, 'subsample': 0.6737137161925263, 'colsample_bytree': 0.45034942302295355, 'reg_alpha': 0.40723204717074263, 'reg_lambda': 0.5957301669529974}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:45:31,880] Trial 20 finished with value: 0.5613511873361365 and parameters: {'n_estimators': 310, 'learning_rate': 0.03223595823440807, 'num_leaves': 71, 'max_depth': 10, 'min_child_samples': 43, 'subsample': 0.7302152057060989, 'colsample_bytree': 0.5552397412219747, 'reg_alpha': 0.19665172623571991, 'reg_lambda': 0.38957368377446494}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:45:57,402] Trial 21 finished with value: 0.5781733321093958 and parameters: {'n_estimators': 1471, 'learning_rate': 0.023304373375872406, 'num_leaves': 81, 'max_depth': 12, 'min_child_samples': 50, 'subsample': 0.643681746501295, 'colsample_bytree': 0.47481644576852444, 'reg_alpha': 0.5207960581944632, 'reg_lambda': 0.6488667950403271}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:46:18,124] Trial 22 finished with value: 0.557763842834386 and parameters: {'n_estimators': 1314, 'learning_rate': 0.02544601193150744, 'num_leaves': 52, 'max_depth': 9, 'min_child_samples': 43, 'subsample': 0.6627380718072275, 'colsample_bytree': 0.5459799592381118, 'reg_alpha': 0.6317229872027623, 'reg_lambda': 0.7693137458646401}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:46:44,186] Trial 23 finished with value: 0.578043569124781 and parameters: {'n_estimators': 1726, 'learning_rate': 0.08165689945172121, 'num_leaves': 87, 'max_depth': 13, 'min_child_samples': 62, 'subsample': 0.600013647171038, 'colsample_bytree': 0.40579860920211547, 'reg_alpha': 0.4503522023827671, 'reg_lambda': 0.610906331644407}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:47:11,358] Trial 24 finished with value: 0.5782144233224403 and parameters: {'n_estimators': 1067, 'learning_rate': 0.018973059957114496, 'num_leaves': 76, 'max_depth': 10, 'min_child_samples': 15, 'subsample': 0.6359854851880032, 'colsample_bytree': 0.45834550035086896, 'reg_alpha': 0.7826989903487098, 'reg_lambda': 0.8873865410525643}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:47:36,714] Trial 25 finished with value: 0.5782037742048073 and parameters: {'n_estimators': 1087, 'learning_rate': 0.08173710589586793, 'num_leaves': 56, 'max_depth': 10, 'min_child_samples': 17, 'subsample': 0.635563476476197, 'colsample_bytree': 0.4552307499243894, 'reg_alpha': 0.7836967637384498, 'reg_lambda': 0.9921628560966806}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:47:58,317] Trial 26 finished with value: 0.5554463277128462 and parameters: {'n_estimators': 751, 'learning_rate': 0.018030670168346245, 'num_leaves': 39, 'max_depth': 8, 'min_child_samples': 14, 'subsample': 0.6897800285084664, 'colsample_bytree': 0.5342729630554042, 'reg_alpha': 0.931167566898202, 'reg_lambda': 0.8766395222688899}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:48:20,620] Trial 27 finished with value: 0.5688634576793724 and parameters: {'n_estimators': 783, 'learning_rate': 0.037196799325750485, 'num_leaves': 71, 'max_depth': 13, 'min_child_samples': 35, 'subsample': 0.7418927709553869, 'colsample_bytree': 0.782160788398872, 'reg_alpha': 0.8226313256301007, 'reg_lambda': 0.9157139814116717}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:48:44,508] Trial 28 finished with value: 0.560250131972181 and parameters: {'n_estimators': 1089, 'learning_rate': 0.05556214390013588, 'num_leaves': 89, 'max_depth': 10, 'min_child_samples': 12, 'subsample': 0.6310854053214295, 'colsample_bytree': 0.6461623900413855, 'reg_alpha': 0.6371461565601958, 'reg_lambda': 0.7986143107043973}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:49:03,751] Trial 29 finished with value: 0.5781482797925506 and parameters: {'n_estimators': 1235, 'learning_rate': 0.017540154203833282, 'num_leaves': 32, 'max_depth': 12, 'min_child_samples': 30, 'subsample': 0.7067522763631156, 'colsample_bytree': 0.43946916801357777, 'reg_alpha': 0.7275716299106498, 'reg_lambda': 0.42519498700187275}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:49:26,640] Trial 30 finished with value: 0.5783008451705465 and parameters: {'n_estimators': 967, 'learning_rate': 0.021811549352438224, 'num_leaves': 55, 'max_depth': 8, 'min_child_samples': 22, 'subsample': 0.8669669465343977, 'colsample_bytree': 0.4013989200007565, 'reg_alpha': 0.5891641314772619, 'reg_lambda': 0.5413879305711692}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:49:48,301] Trial 31 finished with value: 0.5783028623992296 and parameters: {'n_estimators': 956, 'learning_rate': 0.014865910372783085, 'num_leaves': 55, 'max_depth': 8, 'min_child_samples': 21, 'subsample': 0.8745304347281883, 'colsample_bytree': 0.4129703702578582, 'reg_alpha': 0.6044327463096592, 'reg_lambda': 0.5385536718147418}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:50:12,147] Trial 32 finished with value: 0.5780835795285243 and parameters: {'n_estimators': 1156, 'learning_rate': 0.015202879208700558, 'num_leaves': 56, 'max_depth': 7, 'min_child_samples': 22, 'subsample': 0.8762974004831988, 'colsample_bytree': 0.4667298457985667, 'reg_alpha': 0.6088026090524534, 'reg_lambda': 0.48308855930426586}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:50:32,434] Trial 33 finished with value: 0.5558574542345482 and parameters: {'n_estimators': 1023, 'learning_rate': 0.020902385108124987, 'num_leaves': 34, 'max_depth': 7, 'min_child_samples': 21, 'subsample': 0.8765708094731933, 'colsample_bytree': 0.5182101486906907, 'reg_alpha': 0.39124304629426954, 'reg_lambda': 0.41750170043155893}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:50:53,019] Trial 34 finished with value: 0.5613652499074724 and parameters: {'n_estimators': 836, 'learning_rate': 0.015191256385597417, 'num_leaves': 71, 'max_depth': 6, 'min_child_samples': 10, 'subsample': 0.92672050862986, 'colsample_bytree': 0.5775927299259882, 'reg_alpha': 0.901497666613479, 'reg_lambda': 0.561672594377846}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:51:14,243] Trial 35 finished with value: 0.5783147502311933 and parameters: {'n_estimators': 1344, 'learning_rate': 0.02933726874848027, 'num_leaves': 49, 'max_depth': 8, 'min_child_samples': 33, 'subsample': 0.8573309855580636, 'colsample_bytree': 0.44736102707037606, 'reg_alpha': 0.713882509233937, 'reg_lambda': 0.28975165314476653}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:51:35,243] Trial 36 finished with value: 0.5779690480638193 and parameters: {'n_estimators': 711, 'learning_rate': 0.02727685080381675, 'num_leaves': 23, 'max_depth': 8, 'min_child_samples': 33, 'subsample': 0.8050635349574516, 'colsample_bytree': 0.40961128969236094, 'reg_alpha': 0.7077629899506641, 'reg_lambda': 0.2335447950652214}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:51:56,316] Trial 37 finished with value: 0.5779969375055075 and parameters: {'n_estimators': 1242, 'learning_rate': 0.02113724861864472, 'num_leaves': 51, 'max_depth': 6, 'min_child_samples': 23, 'subsample': 0.8546106112463654, 'colsample_bytree': 0.4931368149386077, 'reg_alpha': 0.5910101305847074, 'reg_lambda': 0.33508541029235406}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:52:15,197] Trial 38 finished with value: 0.5777235434275148 and parameters: {'n_estimators': 1328, 'learning_rate': 0.012696373139892053, 'num_leaves': 39, 'max_depth': 5, 'min_child_samples': 33, 'subsample': 0.9287346718675895, 'colsample_bytree': 0.40038339087812425, 'reg_alpha': 0.25283404902185486, 'reg_lambda': 0.26215744675231034}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:52:36,350] Trial 39 finished with value: 0.5603540165692104 and parameters: {'n_estimators': 436, 'learning_rate': 0.010830718300924755, 'num_leaves': 56, 'max_depth': 9, 'min_child_samples': 38, 'subsample': 0.8393145066827911, 'colsample_bytree': 0.8535268311362016, 'reg_alpha': 0.48975932301797154, 'reg_lambda': 0.12323004206062499}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120976 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:52:56,623] Trial 40 finished with value: 0.5781108110027161 and parameters: {'n_estimators': 652, 'learning_rate': 0.03047559887680618, 'num_leaves': 25, 'max_depth': 6, 'min_child_samples': 6, 'subsample': 0.799011327332592, 'colsample_bytree': 0.44016513971895344, 'reg_alpha': 0.6620690199524276, 'reg_lambda': 0.4441959290079066}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:53:19,055] Trial 41 finished with value: 0.5782099078350637 and parameters: {'n_estimators': 1159, 'learning_rate': 0.02069962190112737, 'num_leaves': 73, 'max_depth': 8, 'min_child_samples': 20, 'subsample': 0.8780287799599515, 'colsample_bytree': 0.4589348731361237, 'reg_alpha': 0.78190146373411, 'reg_lambda': 0.7159635623386258}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:53:46,316] Trial 42 finished with value: 0.5780348262145921 and parameters: {'n_estimators': 1025, 'learning_rate': 0.016258965492767872, 'num_leaves': 138, 'max_depth': 9, 'min_child_samples': 25, 'subsample': 0.9066270569866185, 'colsample_bytree': 0.47917110360371545, 'reg_alpha': 0.8194739088823882, 'reg_lambda': 0.5791014940313891}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:54:09,082] Trial 43 finished with value: 0.5780777969394534 and parameters: {'n_estimators': 870, 'learning_rate': 0.028733492262067267, 'num_leaves': 93, 'max_depth': 7, 'min_child_samples': 15, 'subsample': 0.8504893995461952, 'colsample_bytree': 0.43574038999580933, 'reg_alpha': 0.9911990839587658, 'reg_lambda': 0.28883184773910403}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:54:29,362] Trial 44 finished with value: 0.5777211850519971 and parameters: {'n_estimators': 1326, 'learning_rate': 0.012634109246610124, 'num_leaves': 60, 'max_depth': 5, 'min_child_samples': 29, 'subsample': 0.9592274523875223, 'colsample_bytree': 0.43177991595744614, 'reg_alpha': 0.5852306805001906, 'reg_lambda': 0.5381207479661809}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:54:50,504] Trial 45 finished with value: 0.559266096705288 and parameters: {'n_estimators': 1010, 'learning_rate': 0.03351048479948853, 'num_leaves': 51, 'max_depth': 8, 'min_child_samples': 10, 'subsample': 0.7620623217328627, 'colsample_bytree': 0.5244802920941046, 'reg_alpha': 0.7448933113834484, 'reg_lambda': 0.39125866452828983}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:55:22,291] Trial 46 finished with value: 0.5651967780319128 and parameters: {'n_estimators': 1174, 'learning_rate': 0.019390123286124498, 'num_leaves': 67, 'max_depth': 14, 'min_child_samples': 18, 'subsample': 0.810244724931339, 'colsample_bytree': 0.6149753026339297, 'reg_alpha': 0.7055542295547712, 'reg_lambda': 0.09509357408974448}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:55:45,520] Trial 47 finished with value: 0.5780649041941405 and parameters: {'n_estimators': 530, 'learning_rate': 0.02303792582899617, 'num_leaves': 111, 'max_depth': 7, 'min_child_samples': 26, 'subsample': 0.8996878371950288, 'colsample_bytree': 0.4687092711082855, 'reg_alpha': 0.5343806706094251, 'reg_lambda': 0.4717734523150097}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111683 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:56:09,364] Trial 48 finished with value: 0.5781139969768282 and parameters: {'n_estimators': 909, 'learning_rate': 0.014115729003203068, 'num_leaves': 46, 'max_depth': 10, 'min_child_samples': 37, 'subsample': 0.7831655205577044, 'colsample_bytree': 0.4000969809406573, 'reg_alpha': 0.8482081093586721, 'reg_lambda': 0.7128203344601364}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 04:56:27,345] Trial 49 finished with value: 0.5508362550186705 and parameters: {'n_estimators': 1435, 'learning_rate': 0.04137547738297335, 'num_leaves': 76, 'max_depth': 4, 'min_child_samples': 30, 'subsample': 0.8490645266533715, 'colsample_bytree': 0.5029691709752964, 'reg_alpha': 0.7654531168494275, 'reg_lambda': 0.3712398192535}. Best is trial 12 with value: 0.5789410301951485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna search finished.\n",
      "Best trial AUC: 0.5789410301951485\n",
      "Best hyperparameters found:\n",
      "{'n_estimators': 957, 'learning_rate': 0.09035257446504434, 'num_leaves': 61, 'max_depth': 15, 'min_child_samples': 30, 'subsample': 0.7012974416579587, 'colsample_bytree': 0.40894546500107765, 'reg_alpha': 0.4748034675035624, 'reg_lambda': 0.580884129120939}\n",
      "[LightGBM] [Info] Number of positive: 236344, number of negative: 5607100\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5843444, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040446 -> initscore=-3.166501\n",
      "[LightGBM] [Info] Start training from score -3.166501\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.577743\tvalid_0's binary_logloss: 0.169403\n",
      "\n",
      "Trained LightGBM model saved to /kaggle/working/model_assets/lgbm_news_recommender_best_params.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Hyperparameter Tuning with Optuna (Conceptual)\n",
    "\n",
    "# Ensure optuna is installed: !pip install optuna\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Ensure X_train, y_train, X_dev, y_dev are available from Cell 9's context\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt', # Could also try 'dart' or 'goss'\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 2000), # Broader range\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15), # Or -1 for no limit\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0), # Bagging fraction\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0), # Feature fraction\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0), # L2 regularization\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        # 'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 10.0) # If classes imbalanced\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_dev, y_dev)],\n",
    "              eval_metric='auc',\n",
    "              callbacks=[lgb.early_stopping(50, verbose=False)]) # Early stopping within trial\n",
    "\n",
    "    preds_proba = model.predict_proba(X_dev)[:, 1]\n",
    "    auc = roc_auc_score(y_dev, preds_proba)\n",
    "    return auc\n",
    "\n",
    "# Create a study object and specify direction (maximize AUC)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "print(\"Starting Optuna hyperparameter search...\")\n",
    "study.optimize(objective, n_trials=50)  # Number of trials (e.g., 50-100)\n",
    "\n",
    "print(\"\\nOptuna search finished.\")\n",
    "print(f\"Best trial AUC: {study.best_value}\")\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# You would then retrain a final model using these best_params on the full X_train,\n",
    "# potentially with a refined n_estimators based on early stopping from the best trial,\n",
    "# and then save this optimized model.\n",
    "\n",
    "# Example: Retrain with best params\n",
    "best_lgbm_params = study.best_params\n",
    "final_lgbm_model = lgb.LGBMClassifier(**best_lgbm_params)\n",
    "final_lgbm_model.fit(X_train, y_train,\n",
    "                     eval_set=[(X_dev, y_dev)],\n",
    "                     eval_metric='auc',\n",
    "                     callbacks=[lgb.early_stopping(100, verbose=True)])\n",
    "# ... (save this final_lgbm_model)\n",
    "output_path = '/kaggle/working/model_assets/' # Should be defined\n",
    "os.makedirs(output_path, exist_ok=True) # Ensure os is imported and path exists\n",
    "model_filename = os.path.join(output_path, 'lgbm_news_recommender_best_params.pkl')\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(lgbm_model, f)\n",
    "print(f\"\\nTrained LightGBM model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3731d3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:57:29.686778Z",
     "iopub.status.busy": "2025-05-31T04:57:29.686150Z",
     "iopub.status.idle": "2025-05-31T04:57:30.243385Z",
     "shell.execute_reply": "2025-05-31T04:57:30.242275Z"
    },
    "id": "cRl-Jgzusoqg",
    "papermill": {
     "duration": 1.962553,
     "end_time": "2025-05-31T04:57:30.244428",
     "exception": false,
     "start_time": "2025-05-31T04:57:28.281875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model artifacts loaded successfully.\n",
      "Warning: Value 'N21256' not seen by encoder LabelEncoder(). Assigning -1.\n",
      "\n",
      "For sample data: {'UserID': 'U13009', 'ImpressionNewsID': 'N21256', 'History': ['N7074', 'N43029', 'N10307'], 'HistoryImpressionCosineSimilarity': 0.01300215907394886, 'UserID_Encoded': 3346, 'ImpressionNewsID_Encoded': -1}\n",
      "Predicted Click Probability: 0.0274\n",
      "Predicted Click Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Define path where artifacts were saved\n",
    "ARTIFACTS_PATH = '/kaggle/working/model_assets/'\n",
    "\n",
    "# Load TF-IDF vectorizer\n",
    "with open(os.path.join(ARTIFACTS_PATH, 'tfidf_vectorizer.pkl'), 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "# Load NewsID-to-TF-IDF-index mapping\n",
    "with open(os.path.join(ARTIFACTS_PATH, 'newsid_to_tfidf_idx.pkl'), 'rb') as f:\n",
    "    newsid_to_tfidf_idx = pickle.load(f)\n",
    "\n",
    "# Load all_news_tfidf_matrix (or a way to reconstruct it/access news TF-IDF vectors)\n",
    "# For inference, you might just need a way to get a vector for a news_id.\n",
    "# The full matrix might be large. If all_news_for_tfidf_df was saved, you could re-transform.\n",
    "# Or, more practically, news content details would be in a database.\n",
    "# For this example, let's assume we have a way to get TF-IDF for new news.\n",
    "# And let's assume all_news_tfidf_matrix and newsid_to_tfidf_idx cover potential history/candidate news.\n",
    "with open(os.path.join(ARTIFACTS_PATH, 'all_news_tfidf_matrix.pkl'), 'rb') as f:\n",
    "    all_news_tfidf_matrix = pickle.load(f) # Assuming this was saved from Cell 7\n",
    "\n",
    "# Load Label Encoders\n",
    "with open(os.path.join(ARTIFACTS_PATH, 'user_encoder.pkl'), 'rb') as f:\n",
    "    user_encoder = pickle.load(f)\n",
    "with open(os.path.join(ARTIFACTS_PATH, 'news_encoder.pkl'), 'rb') as f:\n",
    "    news_encoder = pickle.load(f)\n",
    "\n",
    "# Load the trained LightGBM model\n",
    "with open(os.path.join(ARTIFACTS_PATH, 'lgbm_news_recommender_best_params.pkl'), 'rb') as f:\n",
    "    lgbm_model = pickle.load(f)\n",
    "\n",
    "print(\"All model artifacts loaded successfully.\")\n",
    "\n",
    "# --- Prepare Sample New Data for Inference ---\n",
    "# This data needs to be in the same format as one row of `train_merged_df` before feature selection\n",
    "# Specifically, it needs: 'UserID', 'ImpressionNewsID', 'History' (list of NewsIDs)\n",
    "# And the news articles (ImpressionNewsID and those in History) need to have their content available\n",
    "# (e.g., 'Title', 'Abstract') if TF-IDF vectors need to be computed for *new* news.\n",
    "\n",
    "# For simplicity, let's assume we have a user and a candidate news article,\n",
    "# and their NewsIDs are known and present in our `newsid_to_tfidf_idx`.\n",
    "\n",
    "sample_inference_data = pd.DataFrame([{\n",
    "    'UserID': 'U13009', # An existing user from the dataset for demo\n",
    "    'ImpressionNewsID': 'N21256', # An existing news item for demo\n",
    "    'History': ['N7074', 'N43029', 'N10307'] # Sample history list\n",
    "    # If news content for ImpressionNewsID or History items is *new* and not in all_news_tfidf_matrix,\n",
    "    # you'd need their 'Title' and 'Abstract' to generate TF-IDF vectors using the loaded tfidf_vectorizer.\n",
    "}])\n",
    "\n",
    "# --- Feature Engineering for the Sample Data (mimicking Cell 8 memory-optimized version) ---\n",
    "num_tfidf_features_inf = all_news_tfidf_matrix.shape[1] # From loaded matrix\n",
    "\n",
    "def get_dense_tfidf_vector_inf(news_id, nid_to_idx_map, matrix, num_features):\n",
    "    if news_id in nid_to_idx_map:\n",
    "        idx = nid_to_idx_map[news_id]\n",
    "        return np.asarray(matrix[idx].todense(), dtype=np.float32).flatten()\n",
    "    # Handle truly new news: if news_id is new, you'd fetch its text,\n",
    "    # transform with tfidf_vectorizer, then convert to dense float32 array.\n",
    "    # For this example, assume all IDs are known from training.\n",
    "    print(f\"Warning: NewsID {news_id} not found in known TF-IDF map during inference. Returning zero vector.\")\n",
    "    return np.zeros(num_features, dtype=np.float32)\n",
    "\n",
    "# Calculate cosine similarity for the sample\n",
    "history_list_inf = sample_inference_data.iloc[0]['History']\n",
    "impression_news_id_inf = sample_inference_data.iloc[0]['ImpressionNewsID']\n",
    "\n",
    "history_item_vectors_inf = []\n",
    "if isinstance(history_list_inf, list) and len(history_list_inf) > 0:\n",
    "    for nid in history_list_inf:\n",
    "        vec = get_dense_tfidf_vector_inf(nid, newsid_to_tfidf_idx, all_news_tfidf_matrix, num_tfidf_features_inf)\n",
    "        if np.any(vec): history_item_vectors_inf.append(vec)\n",
    "\n",
    "if history_item_vectors_inf:\n",
    "    user_profile_vec_inf = np.mean(np.array(history_item_vectors_inf, dtype=np.float32), axis=0)\n",
    "else:\n",
    "    user_profile_vec_inf = np.zeros(num_tfidf_features_inf, dtype=np.float32)\n",
    "\n",
    "impression_vec_inf = get_dense_tfidf_vector_inf(impression_news_id_inf, newsid_to_tfidf_idx, all_news_tfidf_matrix, num_tfidf_features_inf)\n",
    "\n",
    "sim = 0.0\n",
    "if not (np.all(user_profile_vec_inf == 0) or np.all(impression_vec_inf == 0)):\n",
    "    # Need to import cosine_similarity from sklearn.metrics.pairwise\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sim = cosine_similarity(user_profile_vec_inf.reshape(1, -1), impression_vec_inf.reshape(1, -1))[0,0]\n",
    "sample_inference_data['HistoryImpressionCosineSimilarity'] = [sim]\n",
    "\n",
    "# Encode UserID and ImpressionNewsID\n",
    "# Handle unknown labels: if an ID wasn't seen during training, encoder will fail.\n",
    "# Strategy: map unknown to a special category or ignore. For simplicity, assume known.\n",
    "def safe_transform(encoder, value, unknown_value=-1): # -1 or some other placeholder\n",
    "    try:\n",
    "        return encoder.transform([str(value)])[0]\n",
    "    except ValueError: # Or more specific: NotFittedError or if value not in classes_\n",
    "        print(f\"Warning: Value '{value}' not seen by encoder {encoder}. Assigning {unknown_value}.\")\n",
    "        # Check if unknown_value is a valid encoded value or if the model can handle it.\n",
    "        # A robust way is to add an \"unknown\" category to encoders during training.\n",
    "        return unknown_value # This might break model if it expects specific range\n",
    "\n",
    "sample_inference_data['UserID_Encoded'] = safe_transform(user_encoder, sample_inference_data.iloc[0]['UserID'])\n",
    "sample_inference_data['ImpressionNewsID_Encoded'] = safe_transform(news_encoder, sample_inference_data.iloc[0]['ImpressionNewsID'])\n",
    "\n",
    "# --- Make Prediction ---\n",
    "inference_features = ['UserID_Encoded', 'ImpressionNewsID_Encoded', 'HistoryImpressionCosineSimilarity']\n",
    "X_inference = sample_inference_data[inference_features]\n",
    "\n",
    "predicted_click_probability = lgbm_model.predict_proba(X_inference)[:, 1]\n",
    "predicted_click_label = lgbm_model.predict(X_inference)\n",
    "\n",
    "print(f\"\\nFor sample data: {sample_inference_data.iloc[0].to_dict()}\")\n",
    "print(f\"Predicted Click Probability: {predicted_click_probability[0]:.4f}\")\n",
    "print(f\"Predicted Click Label: {predicted_click_label[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7cad5",
   "metadata": {
    "id": "QrzkslvLuybp",
    "papermill": {
     "duration": 1.438702,
     "end_time": "2025-05-31T04:57:33.114875",
     "exception": false,
     "start_time": "2025-05-31T04:57:31.676173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7552513,
     "sourceId": 12005530,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5518.665695,
   "end_time": "2025-05-31T04:57:37.661083",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-31T03:25:38.995388",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
